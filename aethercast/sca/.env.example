# --- AIMS Service Configuration ---
# URL for the AIMS (AI Model Service) for snippet generation - derived from common.env
AIMS_SERVICE_URL=${AIMS_SERVICE_BASE_URL_CONTAINER}/v1/generate
# Timeout in seconds for requests to the AIMS service
AIMS_REQUEST_TIMEOUT_SECONDS=60

# --- LLM Parameters (These are now requests to AIMS) ---
# SCA_LLM_MODEL_ID: The Model ID to request from AIMS for snippet generation.
# Example: gpt-3.5-turbo, or custom model IDs AIMS might support.
SCA_LLM_MODEL_ID=gpt-3.5-turbo

# SCA_LLM_MAX_TOKENS_SNIPPET: Maximum number of tokens for the generated snippet (passed to AIMS).
SCA_LLM_MAX_TOKENS_SNIPPET=150

# SCA_LLM_TEMPERATURE_SNIPPET: Sampling temperature for the LLM (passed to AIMS).
SCA_LLM_TEMPERATURE_SNIPPET=0.7

# --- SCA Behavior Configuration ---
# USE_REAL_LLM_SERVICE: Set to 'true' to use the real LLM service (via AIMS),
# 'false' for placeholder/simulated responses (bypasses AIMS).
# For integration tests, 'false' is recommended.
USE_REAL_LLM_SERVICE=false

# --- Flask App Parameters (SCA runs on port 5002 by default in docker-compose.yml) ---
# FLASK_RUN_HOST=0.0.0.0
# FLASK_RUN_PORT=5002
# FLASK_DEBUG=True # Standard Flask debug mode, picked up by `flask run` or `app.run(debug=True)`
SCA_HOST=0.0.0.0
SCA_PORT=5002
# SCA_DEBUG_MODE=True # If using a custom debug flag in main.py instead of FLASK_DEBUG for app.run()
