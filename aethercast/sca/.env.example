# SCA_LLM_PROVIDER: Specifies the LLM provider (e.g., 'openai', 'azure_openai', 'custom_service')
SCA_LLM_PROVIDER=openai

# SCA_LLM_API_KEY: Your API key for the LLM service. (Required if USE_REAL_LLM_SERVICE=true)
# For integration tests, leave blank if USE_REAL_LLM_SERVICE=false.
SCA_LLM_API_KEY=your_llm_api_key_here

# SCA_LLM_BASE_URL: The base URL for the LLM service API. (Required if USE_REAL_LLM_SERVICE=true)
# Example for OpenAI: https://api.openai.com/v1
SCA_LLM_BASE_URL=https://api.openai.com/v1

# SCA_LLM_MODEL_ID: The specific model ID to use for snippet generation. (Required if USE_REAL_LLM_SERVICE=true)
# Example for OpenAI: gpt-3.5-turbo
SCA_LLM_MODEL_ID=gpt-3.5-turbo

# SCA_LLM_MAX_TOKENS_SNIPPET: Maximum number of tokens for the generated snippet.
SCA_LLM_MAX_TOKENS_SNIPPET=150

# SCA_LLM_TEMPERATURE_SNIPPET: Sampling temperature for the LLM.
SCA_LLM_TEMPERATURE_SNIPPET=0.7

# SCA_LLM_REQUEST_TIMEOUT_SECONDS: Timeout in seconds for requests to the LLM service.
SCA_LLM_REQUEST_TIMEOUT_SECONDS=30

# USE_REAL_LLM_SERVICE: Set to 'true' to use the real LLM service, 'false' for placeholder/simulated responses.
# For integration tests, 'false' is recommended.
USE_REAL_LLM_SERVICE=false

# AIMS_LLM_PLACEHOLDER_URL: URL for the placeholder LLM service (currently not actively called by SCA if USE_REAL_LLM_SERVICE=false, as response is simulated internally).
# Included for completeness if the placeholder logic were to change to make a live call.
# AIMS_LLM_PLACEHOLDER_URL=http://localhost:8000/v1/generate

# Flask app parameters (SCA runs on port 5002 by default in docker-compose.yml)
# FLASK_RUN_HOST=0.0.0.0
# FLASK_RUN_PORT=5002
# FLASK_DEBUG=True
