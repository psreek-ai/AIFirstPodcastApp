# --- AIMS Service Configuration ---
# URL for the AIMS (AI Model Service) - derived from common.env
AIMS_SERVICE_URL=${AIMS_SERVICE_BASE_URL_CONTAINER}/v1/generate
# Timeout in seconds for requests to the AIMS service
AIMS_REQUEST_TIMEOUT_SECONDS=180

# --- LLM Parameters (These are now requests to AIMS) ---
# PSWA_LLM_MODEL: The Model ID to request from AIMS.
# Examples: gpt-3.5-turbo-0125, gpt-4-turbo-preview, or custom model IDs AIMS might support.
PSWA_LLM_MODEL=gpt-3.5-turbo-0125

# PSWA_LLM_TEMPERATURE: Temperature for the LLM response (0.0 to 2.0).
PSWA_LLM_TEMPERATURE=0.7

# PSWA_LLM_MAX_TOKENS: Maximum tokens to generate in the LLM response.
PSWA_LLM_MAX_TOKENS=1500

# PSWA_LLM_JSON_MODE: Set to 'true' to request JSON output from AIMS (if the model supports it).
# If 'true', ensure system/user prompts guide the LLM for JSON output.
# If 'false' or if the model doesn't support JSON mode flag, PSWA will rely on tag-based parsing from text.
PSWA_LLM_JSON_MODE=true

# --- Prompt Engineering ---
# PSWA_DEFAULT_PROMPT_SYSTEM_MESSAGE: System message for the LLM.
# This example is for JSON mode. Adjust if not using JSON mode or for different structures.
PSWA_DEFAULT_PROMPT_SYSTEM_MESSAGE="You are a podcast scriptwriter. Your output MUST be a single, valid JSON object. Do not include any text outside of this JSON object, not even markdown tags like ```json. The JSON object should conform to the following schema: {\"title\": \"string\", \"intro\": \"string\", \"segments\": [{\"segment_title\": \"string\", \"content\": \"string\"}], \"outro\": \"string\"}. If content is insufficient, return JSON: {\"error\": \"Insufficient content\", \"message\": \"Details... for topic: [topic_name_here]\"}."

# PSWA_DEFAULT_PROMPT_USER_TEMPLATE: Template for the user message to the LLM.
# Use {topic} and {content} as placeholders. This example is for JSON mode.
PSWA_DEFAULT_PROMPT_USER_TEMPLATE="Generate a podcast script for topic '{topic}' using the following content:\n---\n{content}\n---\nRemember, your entire response must be a single JSON object conforming to the schema provided in the system message."

# --- Script Caching Configuration ---
# SHARED_DATABASE_PATH: Path to the shared SQLite database file for script caching.
# This should be the SAME path used by the API Gateway.
# When running with Docker Compose, this should reference the var from common.env:
# Example: ${SHARED_DATABASE_PATH}
SHARED_DATABASE_PATH=${SHARED_DATABASE_PATH}

# PSWA_SCRIPT_CACHE_ENABLED: Set to 'true' to enable script caching, 'false' to disable.
PSWA_SCRIPT_CACHE_ENABLED=true

# PSWA_SCRIPT_CACHE_MAX_AGE_HOURS: Maximum age of a cached script in hours before it's considered stale.
# 720 hours = 30 days
PSWA_SCRIPT_CACHE_MAX_AGE_HOURS=720

# --- Test Mode for Integration Testing ---
# PSWA_TEST_MODE_ENABLED: Set to 'true' to bypass LLM calls (including AIMS) and return a dummy script.
# Recommended for integration tests to avoid external dependencies and costs.
PSWA_TEST_MODE_ENABLED=true

# --- Flask App Parameters ---
# PSWA_HOST=0.0.0.0
# PSWA_PORT=5004
# PSWA_DEBUG_MODE=True # For direct Flask run, Docker Compose typically uses FLASK_DEBUG
# FLASK_DEBUG=True # Standard Flask debug mode, picked up by `flask run` or `app.run(debug=True)`
