import flask
import uuid
import datetime
import logging
import json
import requests # For calling AIMS (LLM)
import re

app = flask.Flask(__name__)

# --- Configuration & Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- AIMS (LLM) Placeholder Configuration ---
AIMS_LLM_PLACEHOLDER_URL = "http://localhost:8000/v1/generate" # Assuming AIMS LLM placeholder runs here
# This is the hardcoded response from aethercast/aims/llm_api_placeholder.md
# Modified slightly to represent a script structure for easier parsing by PSWA.
AIMS_LLM_SCRIPT_GENERATION_HARDCODED_RESPONSE = {
  "request_id": "aims-llm-placeholder-req-pswa-789",
  "model_id": "AetherLLM-Placeholder-Script-v0.1",
  "choices": [
    {
      "text": """
[TITLE] A Deep Dive into Placeholder Technologies
[INTRO]
Welcome to "A Deep Dive into Placeholder Technologies"! In today's episode, we'll explore the fascinating world of placeholder systems, how they're used, and why they are so crucial in development. We'll be looking at insights from various sources.
[SEGMENT_1_TITLE] Understanding Placeholders
[SEGMENT_1_CONTENT]
Placeholders, in essence, are temporary stand-ins for content or functionality that is not yet available. They allow developers and designers to build and test systems without waiting for all components to be finalized. For example, this very script is generated by a placeholder LLM, demonstrating the concept meta-style! The input articles discussed topics like AI, quantum computing, and renewable energy, all very complex subjects where placeholders are often used in their early documentation and UI mockups.
[SEGMENT_2_TITLE] The Importance of Simulated Data
[SEGMENT_2_CONTENT]
When building complex applications, such as the Aethercast system itself, having realistic simulated data is key. This data, much like the content from our simulated WebContentHarvesterAgent, helps ensure that downstream components, like this PodcastScriptWeaverAgent, can be developed and tested effectively. It helps identify integration points and potential issues early on. For instance, if the WCHA provided poorly structured data, this agent would struggle to create a coherent script prompt for the LLM.
[OUTRO]
So, as we've seen, placeholder technologies and simulated data are not just trivial stand-ins; they are fundamental tools that enable parallel development, robust testing, and smoother integration. That's all for this episode of "A Deep Dive into Placeholder Technologies". Thanks for listening!
""",
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 150, 
    "completion_tokens": 250, 
    "total_tokens": 400
  }
}
# For actual interaction with a running AIMS placeholder, set this to True
SIMULATE_AIMS_LLM_CALL = False


# --- Helper Functions ---
def generate_script_id() -> str:
    """Generates a unique script ID."""
    return f"script_{uuid.uuid4().hex[:12]}"

def estimate_reading_time_seconds(text: str) -> int:
    """Estimates reading time in seconds (words per minute / 60)."""
    if not text: return 0
    words_per_minute = 150 # Average reading speed
    words = len(text.split())
    return int((words / words_per_minute) * 60)

def call_aims_llm_for_script(prompt: str, context_articles: list) -> dict:
    """
    Simulates calling the AIMS LLM placeholder for script generation or calls it if SIMULATE_AIMS_LLM_CALL is True.
    """
    logging.info(f"[PSWA_AIMS_CALL] Calling AIMS LLM for script generation. Prompt length: {len(prompt)}")
    
    if SIMULATE_AIMS_LLM_CALL:
        payload = {
            "model_id": "AetherLLM-PodcastScript-v1", # Placeholder model for script generation
            "prompt": prompt,
            "max_tokens": 1000, # Longer for a full script
            "temperature": 0.65,
            "context": {
                "source_article_count": len(context_articles),
                "article_titles": [article.get("title", "N/A") for article in context_articles[:3]] # First 3 titles
            },
            "response_format": "text" # Expecting a structured text response
        }
        try:
            response = requests.post(AIMS_LLM_PLACEHOLDER_URL, json=payload, timeout=30) # Longer timeout for script
            response.raise_for_status()
            llm_response = response.json()
            logging.info(f"[PSWA_AIMS_CALL_SUCCESS] Received response from AIMS LLM.")
            return llm_response
        except requests.exceptions.RequestException as e:
            logging.error(f"[PSWA_AIMS_CALL_ERROR] Error calling AIMS LLM: {e}. Falling back to hardcoded response.")
            return AIMS_LLM_SCRIPT_GENERATION_HARDCODED_RESPONSE # Fallback
        except json.JSONDecodeError as e:
            logging.error(f"[PSWA_AIMS_CALL_ERROR] Error decoding JSON from AIMS LLM: {e}. Falling back to hardcoded response.")
            return AIMS_LLM_SCRIPT_GENERATION_HARDCODED_RESPONSE # Fallback
    else:
        logging.info("[PSWA_AIMS_CALL] Dynamically generating AIMS LLM script response (SIMULATE_AIMS_LLM_CALL is False).")
        import time
        time.sleep(0.1)

        # Extract podcast_title_suggestion from the prompt.
        # The prompt structure is "You are... Podcast Title: \"{podcast_title_suggestion}\"..."
        title_suggestion_match = re.search(r"Podcast Title: \"(.*?)\"", prompt)
        suggested_title = "A Deep Dive into Placeholder Technologies" # Default
        if title_suggestion_match:
            suggested_title = title_suggestion_match.group(1)
        
        # Dynamically create the response text
        # Replace the [TITLE] content in the hardcoded response text
        original_script_text = AIMS_LLM_SCRIPT_GENERATION_HARDCODED_RESPONSE["choices"][0]["text"]
        dynamic_script_text = re.sub(r"\[TITLE\].*?\n", f"[TITLE] {suggested_title}\n", original_script_text, count=1, flags=re.IGNORECASE)

        # Create a new response object, copying structure but with the new dynamic text
        response = json.loads(json.dumps(AIMS_LLM_SCRIPT_GENERATION_HARDCODED_RESPONSE)) # Deep copy
        response["choices"][0]["text"] = dynamic_script_text
        response["request_id"] = f"aims-llm-placeholder-req-dynamic-script-{uuid.uuid4().hex[:6]}"
        response["model_id"] = "AetherLLM-Placeholder-DynamicScript-v0.2"
        
        response["usage"]["prompt_tokens"] = len(prompt.split()) // 4 
        response["usage"]["completion_tokens"] = len(dynamic_script_text.split()) // 4
        response["usage"]["total_tokens"] = response["usage"]["prompt_tokens"] + response["usage"]["completion_tokens"]
        
        return response

def parse_llm_script_text(script_text: str) -> tuple[str, list]:
    """
    Parses the LLM-generated script text into a title and segments.
    This parser is specific to the format used in AIMS_LLM_SCRIPT_GENERATION_HARDCODED_RESPONSE.
    Format:
    [TITLE] Actual Title
    [INTRO]
    Intro content...
    [SEGMENT_X_TITLE] Segment X Title
    [SEGMENT_X_CONTENT]
    Segment X content...
    [OUTRO]
    Outro content...
    """
    script_title = "Default Podcast Title"
    segments = []
    
    # Extract title
    title_match = re.search(r"\[TITLE\](.*?)\n", script_text, re.IGNORECASE)
    if title_match:
        script_title = title_match.group(1).strip()

    # Extract intro
    intro_match = re.search(r"\[INTRO\]\n(.*?)(?=\n\[SEGMENT_|\n\[OUTRO\])", script_text, re.DOTALL | re.IGNORECASE)
    if intro_match:
        segments.append({"segment_title": "Introduction", "script_content": intro_match.group(1).strip()})

    # Extract main segments
    for match in re.finditer(r"\[SEGMENT_(\d+)_TITLE\](.*?)\n\[SEGMENT_\1_CONTENT\]\n(.*?)(?=\n\[SEGMENT_|\n\[OUTRO\])", script_text, re.DOTALL | re.IGNORECASE):
        segment_num = match.group(1)
        title = match.group(2).strip()
        content = match.group(3).strip()
        segments.append({"segment_title": title if title else f"Main Segment {segment_num}", "script_content": content})
        
    # Extract outro
    outro_match = re.search(r"\[OUTRO\]\n(.*)", script_text, re.DOTALL | re.IGNORECASE)
    if outro_match:
        segments.append({"segment_title": "Outro", "script_content": outro_match.group(1).strip()})

    if not segments and script_text: # Fallback if parsing fails but text exists
        logging.warning("Failed to parse script segments with specific tags. Using full text as one segment.")
        segments.append({"segment_title": "Full Script", "script_content": script_text.strip()})
        
    return script_title, segments


# --- API Endpoint ---
@app.route("/weave_script", methods=["POST"])
def weave_script_endpoint():
    """
    API endpoint for CPOA to request podcast script generation.
    Accepts JSON payload with:
    - 'retrieved_content': dict (from WCHA, expected to have 'retrieved_articles' list)
    - 'podcast_title_suggestion': string
    - 'podcast_style': string (e.g., "informative", "conversational")
    - 'topic_id': string (optional)
    """
    try:
        request_data = flask.request.get_json()
        if not request_data:
            return flask.jsonify({"error": "Invalid JSON payload"}), 400

        retrieved_content = request_data.get("retrieved_content", {"retrieved_articles": []})
        podcast_title_suggestion = request_data.get("podcast_title_suggestion", "Untitled Podcast")
        podcast_style = request_data.get("podcast_style", "informative")
        topic_id = request_data.get("topic_id") # Optional
        error_trigger = request_data.get("error_trigger")

        logging.info(f"[PSWA_REQUEST] Received /weave_script. Title: '{podcast_title_suggestion}', Style: '{podcast_style}', ErrorTrigger: '{error_trigger}'")
        
        if error_trigger == "pswa_error":
            logging.warning(f"[PSWA_SIMULATED_ERROR] Simulating an error for /weave_script based on error_trigger: {error_trigger}")
            return flask.jsonify({
                "error": "Simulated PSWA Error",
                "details": "This is a controlled error triggered for testing purposes in PodcastScriptWeaverAgent."
            }), 500

        articles = retrieved_content.get("retrieved_articles", [])
        if not articles:
            logging.warning("[PSWA_REQUEST] No articles provided in retrieved_content. Script will be very generic.")
        
        # 1. Formulate Prompt for AIMS LLM
        prompt_parts = [
            "You are a podcast scriptwriter. Your task is to generate a compelling podcast script.",
            f"The desired podcast title is: \"{podcast_title_suggestion}\".",
            f"The style/tone should be: {podcast_style}."
        ]

        if articles and isinstance(articles, list) and len(articles) > 0:
            prompt_parts.append("\nKey information gathered from web sources includes:")
            for i, article in enumerate(articles[:min(len(articles), 3)]): # Summarize first 3 articles
                article_title = article.get('title', 'Untitled Source')
                # Prefer 'summary' if available and good, else fallback to start of 'text_content'
                article_text_snippet = article.get('summary', article.get('text_content', 'No content preview available.'))
                if len(article_text_snippet) > 200: # Keep snippets relatively short for the prompt
                    article_text_snippet = article_text_snippet[:200] + "..."
                
                prompt_parts.append(f"\nSource {i+1}: \"{article_title}\"")
                prompt_parts.append(f"Content snippet: \"{article_text_snippet}\"")
            if len(articles) > 3:
                prompt_parts.append(f"\n(Plus {len(articles) - 3} more sources...)")
            prompt_parts.append("\nPlease synthesize insights from these sources to create the script segments.")
        else:
            prompt_parts.append("\nNo specific web content was provided. Please generate a general script based on the title and style.")

        prompt_parts.append("""
\nInstructions for Script Output:
- Create a script with a clear structure: an introduction, 2-3 main segments discussing different aspects or key points from the content, and an outro.
- The script should be engaging and easy to follow.
- Ensure the language is appropriate for a spoken podcast.
- Output the script in the following format, with each section clearly marked:
[TITLE] The Final Title You Decide For The Podcast (can be same as suggestion)
[INTRO]
(Introductory content here)
[SEGMENT_1_TITLE] Title for Segment 1
[SEGMENT_1_CONTENT]
(Content for Segment 1 here)
[SEGMENT_2_TITLE] Title for Segment 2
[SEGMENT_2_CONTENT]
(Content for Segment 2 here)
... (add more segments if logical, up to 3 main ones)
[OUTRO]
(Outro content here)""")
        prompt = "\n".join(prompt_parts)

        # 2. Call AIMS LLM Placeholder
- Create a script with a clear structure: an introduction, 2-3 main segments discussing different aspects or key points from the content, and an outro.
- The script should be engaging and easy to follow.
- If multiple sources are summarized above, try to synthesize information from them into coherent segments.
- Ensure the language is appropriate for a spoken podcast.
- Output the script in the following format, with each section clearly marked:
[TITLE] The Final Title You Decide For The Podcast (can be same as suggestion)
[INTRO]
(Introductory content here)
[SEGMENT_1_TITLE] Title for Segment 1
[SEGMENT_1_CONTENT]
(Content for Segment 1 here)
[SEGMENT_2_TITLE] Title for Segment 2
[SEGMENT_2_CONTENT]
(Content for Segment 2 here)
... (add more segments if logical, up to 3 main ones)
[OUTRO]
(Outro content here)
"""
        # 2. Call AIMS LLM Placeholder
        llm_response = call_aims_llm_for_script(prompt, articles)
        raw_script_text = llm_response.get("choices", [{}])[0].get("text", "Error: LLM response format for script unexpected.")

        # 3. Parse LLM Response and Structure PodcastScript
        final_script_title, segments = parse_llm_script_text(raw_script_text)
        
        full_text_for_estimation = "\n".join([seg.get("script_content","") for seg in segments])
        estimated_duration = estimate_reading_time_seconds(full_text_for_estimation)
        
        script_id = generate_script_id()
        timestamp = datetime.datetime.utcnow().isoformat() + "Z"

        # This structure matches CPOA's `call_podcast_script_weaver_agent` current expectation
        # (podcast_id, title, script as list of dicts)
        # And also aligns with the `PodcastScript` from docs/architecture/AI_Agents_Overview.md
        podcast_script_object = {
            "podcast_id": script_id, # Using script_id as podcast_id for CPOA
            "title": final_script_title, # Title from LLM or default
            "script": segments, # List of {"segment_title": ..., "script_content": ...}
            # --- Additional fields from PodcastScript spec ---
            "script_id": script_id, 
            "topic_id": topic_id, # Pass through if provided
            "script_title_suggestion": final_script_title,
            "full_text_script": raw_script_text, # Full raw output from LLM
            "estimated_reading_time_seconds": estimated_duration,
            "persona_used": podcast_style, # Using style as persona for now
            "generation_timestamp": timestamp,
            "llm_prompt_used": prompt,
            "llm_model_used": llm_response.get("model_id", "unknown")
        }
        
        logging.info(f"[PSWA_RESPONSE] Podcast script '{script_id}' woven successfully for title '{final_script_title}'.")
        return flask.jsonify(podcast_script_object), 200

    except Exception as e:
        logging.error(f"Error in /weave_script endpoint: {e}", exc_info=True)
        return flask.jsonify({"error": f"Internal server error in PSWA: {str(e)}"}), 500

if __name__ == "__main__":
    # Run PSWA on a different port
    # Example: python aethercast/pswa/main.py
    app.run(host="0.0.0.0", port=5004, debug=True)
```
